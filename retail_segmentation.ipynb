{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fbab18",
   "metadata": {},
   "source": [
    "# DSII — Retail Customer Segmentation & Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bddb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Directories\n",
    "from pathlib import Path\n",
    "BASE  = Path.cwd()\n",
    "DATA  = BASE / \"data\"\n",
    "PROC  = DATA / \"processed\"\n",
    "FIGS  = BASE / \"reports\" / \"figures\"\n",
    "\n",
    "for p in [DATA, PROC, FIGS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_CSV = BASE / \"online_retail_II.csv\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81759d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK → {\n",
      "  \"random_seed\": 1337,\n",
      "  \"paths\": {\n",
      "    \"processed_dir\": \"C:\\\\Users\\\\mttng\\\\Downloads\\\\retail-segmentation-anomalies\\\\data\\\\processed\",\n",
      "    \"figures_dir\": \"C:\\\\Users\\\\mttng\\\\Downloads\\\\retail-segmentation-anomalies\\\\reports\\\\figures\"\n",
      "  },\n",
      "  \"clustering\": {\n",
      "    \"kmeans\": {\n",
      "      \"kmin\": 2,\n",
      "      \"kmax\": 10,\n",
      "      \"n_init\": 10\n",
      "    }\n",
      "  },\n",
      "  \"projection\": {\n",
      "    \"method\": \"umap\",\n",
      "    \"random_state\": 1337\n",
      "  },\n",
      "  \"seq\": {\n",
      "    \"seq_len\": 50,\n",
      "    \"max_vocab\": 2000,\n",
      "    \"min_count\": 5,\n",
      "    \"batch\": 256,\n",
      "    \"epochs\": 10\n",
      "  },\n",
      "  \"ae\": {\n",
      "    \"bottleneck\": 8,\n",
      "    \"l2\": 0.0001,\n",
      "    \"dropout\": 0.1,\n",
      "    \"epochs\": 200,\n",
      "    \"batch\": 256,\n",
      "    \"patience\": 10\n",
      "  }\n",
      "}\n",
      "Data: C:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\online_retail_II.csv\n",
      "Outputs → C:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\data\\processed and C:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\reports\\figures\n"
     ]
    }
   ],
   "source": [
    "# Setup Enviornment\n",
    "import os, random, json, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED); os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\n",
    "\n",
    "# Paths (relative to notebook)\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATASET_PATH = ROOT / \"online_retail_II.csv\"  # <-- keep CSV next to notebook\n",
    "PROC = ROOT / \"data\" / \"processed\"\n",
    "FIGS = ROOT / \"reports\" / \"figures\"\n",
    "PROC.mkdir(parents=True, exist_ok=True); FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {\"processed_dir\": str(PROC), \"figures_dir\": str(FIGS)},\n",
    "    \"clustering\": {\"kmeans\": {\"kmin\": 2, \"kmax\": 10, \"n_init\": 10}},\n",
    "    \"projection\": {\"method\": \"umap\", \"random_state\": RANDOM_SEED},\n",
    "    \"seq\": {\"seq_len\": 50, \"max_vocab\": 2000, \"min_count\": 5, \"batch\": 256, \"epochs\": 10},\n",
    "    \"ae\": {\"bottleneck\": 8, \"l2\": 1e-4, \"dropout\": 0.10, \"epochs\": 200, \"batch\": 256, \"patience\": 10},\n",
    "}\n",
    "\n",
    "print(\"Config OK →\", json.dumps(CFG, indent=2))\n",
    "print(\"Data:\", DATASET_PATH)\n",
    "print(\"Outputs →\", PROC, \"and\", FIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d94b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility pathing for figures & timers\n",
    "def savefig(name, tight=True, dpi=200):\n",
    "    if tight: plt.tight_layout()\n",
    "    out = Path(CFG[\"paths\"][\"figures_dir\"]) / name\n",
    "    plt.savefig(out, dpi=dpi)\n",
    "    plt.close()\n",
    "    return out\n",
    "\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer(msg):\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    print(f\"[start] {msg}\")\n",
    "    yield\n",
    "    print(f\"[done]  {msg} in {time.time()-t0:.2f}s\")\n",
    "\n",
    "def ensure_cols(df):\n",
    "    # tolerant column mapper\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    def find(*cands):\n",
    "        for k in cands:\n",
    "            if k in cols: return cols[k]\n",
    "        return None\n",
    "    mapping = {\n",
    "        \"Invoice\": find(\"invoice\",\"invoiceno\",\"invoice no\",\"invoicenumber\"),\n",
    "        \"InvoiceDate\": find(\"invoicedate\",\"date\"),\n",
    "        \"CustomerID\": find(\"customer id\",\"customerid\",\"custid\",\"cust id\"),\n",
    "        \"StockCode\": find(\"stockcode\",\"sku\",\"productcode\",\"product code\"),\n",
    "        \"Quantity\": find(\"quantity\",\"qty\",\"count\"),\n",
    "        \"Price\": find(\"price\",\"unitprice\",\"unit price\"),\n",
    "        \"Description\": find(\"description\",\"desc\")\n",
    "    }\n",
    "    missing = [k for k,v in mapping.items() if v is None and k not in [\"Description\"]]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c623672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Poster style (once per notebook) ----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"font.size\": 12,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"white\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b808ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] Load CSV\n",
      "[done]  Load CSV in 0.86s\n",
      "Rows after cleaning: 805549\n",
      "Unique customers: 5878\n",
      "Date range: 2009-12-01 07:45:00 → 2011-12-09 12:50:00\n"
     ]
    }
   ],
   "source": [
    "# Loading & Cleaning Data\n",
    "with timer(\"Load CSV\"):\n",
    "    raw = pd.read_csv(DATASET_PATH, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "colmap = ensure_cols(raw)\n",
    "raw = raw.rename(columns={v:k for k,v in colmap.items() if v is not None})\n",
    "\n",
    "# Parse dates, compute line totals\n",
    "raw[\"InvoiceDate\"] = pd.to_datetime(raw[\"InvoiceDate\"], errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[\"InvoiceDate\"])\n",
    "raw[\"LineTotal\"] = raw[\"Quantity\"] * raw[\"Price\"]\n",
    "\n",
    "# Remove cancellations/returns (Invoice starts with 'C'), invalid rows\n",
    "mask_cancel = raw[\"Invoice\"].astype(str).str.startswith(\"C\")\n",
    "raw = raw.loc[~mask_cancel].copy()\n",
    "raw = raw[(raw[\"Quantity\"] > 0) & (raw[\"Price\"] > 0)]\n",
    "raw = raw.dropna(subset=[\"CustomerID\"])\n",
    "raw[\"CustomerID\"] = raw[\"CustomerID\"].astype(str)\n",
    "\n",
    "print(\"Rows after cleaning:\", len(raw))\n",
    "print(\"Unique customers:\", raw[\"CustomerID\"].nunique())\n",
    "print(\"Date range:\", raw[\"InvoiceDate\"].min(), \"→\", raw[\"InvoiceDate\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011b63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 805,549\n",
      "Unique customers: 5,878\n",
      "Date range: 2009-12-01 07:45:00 → 2011-12-09 12:50:00\n",
      "Features table: 5,878 customers, columns = ['customerid', 'tx_count', 'spend_sum', 'item_qty_sum', 'recency_days', 'basket_size_mean', 'RFM_Score']\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering (RFM + interpretable features)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load CSV\n",
    "CSV_PATH = Path(globals().get(\"DATASET_PATH\", \"online_retail_II.csv\"))\n",
    "raw = pd.read_csv(CSV_PATH, encoding_errors=\"ignore\")\n",
    "\n",
    "# 2. Harmonize column names across variants\n",
    "def rename_first_match(df, std, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df if c == std else df.rename(columns={c: std})\n",
    "    raise KeyError(f\"Required column '{std}' not found. Looked for {candidates}. Got {list(df.columns)[:20]} ...\")\n",
    "\n",
    "df = raw.copy()\n",
    "df = rename_first_match(df, \"CustomerID\",  [\"CustomerID\",\"Customer ID\",\"customerid\"])\n",
    "df = rename_first_match(df, \"InvoiceNo\",   [\"InvoiceNo\",\"Invoice\",\"Invoice No\"])\n",
    "df = rename_first_match(df, \"InvoiceDate\", [\"InvoiceDate\",\"Invoice Date\",\"date\",\"Date\"])\n",
    "df = rename_first_match(df, \"Price\",       [\"Price\",\"UnitPrice\",\"Unit Price\"])\n",
    "df = rename_first_match(df, \"Quantity\",    [\"Quantity\",\"Qty\",\"quantity\"])\n",
    "df = rename_first_match(df, \"StockCode\",   [\"StockCode\",\"Stock Code\",\"SKU\",\"ProductCode\",\"Product Code\",\"Description\"])\n",
    "\n",
    "# 3. Strong typing & cleaning\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df[\"InvoiceNo\"]   = df[\"InvoiceNo\"].astype(str).str.strip()\n",
    "df[\"CustomerID\"]  = pd.to_numeric(df[\"CustomerID\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "# Drop cancels/returns (InvoiceNo that start with 'C'), invalid rows, and nulls\n",
    "before = len(df)\n",
    "df = df[~df[\"InvoiceNo\"].str.startswith(\"C\", na=False)]\n",
    "df = df.dropna(subset=[\"CustomerID\",\"InvoiceDate\"]).copy()\n",
    "df = df[(df[\"Quantity\"] > 0) & (df[\"Price\"] > 0)].copy()\n",
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"int64\")\n",
    "\n",
    "# 4. Line total\n",
    "if \"LineTotal\" not in df.columns:\n",
    "    df[\"LineTotal\"] = df[\"Quantity\"] * df[\"Price\"]\n",
    "\n",
    "# 5. Per-invoice basket stats\n",
    "basket_per_invoice = (\n",
    "    df.groupby([\"InvoiceNo\",\"CustomerID\"], observed=True)[\"StockCode\"]\n",
    "      .count().reset_index(name=\"items_per_invoice\")\n",
    ")\n",
    "\n",
    "# 6. Customer-level features\n",
    "g = df.groupby(\"CustomerID\", observed=True)\n",
    "cust = (\n",
    "    g.agg(\n",
    "        tx_count     = (\"InvoiceNo\",\"nunique\"),\n",
    "        spend_sum    = (\"LineTotal\",\"sum\"),\n",
    "        item_qty_sum = (\"Quantity\",\"sum\"),\n",
    "        last_date    = (\"InvoiceDate\",\"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"CustomerID\":\"customerid\"})\n",
    ")\n",
    "\n",
    "# Recency in days\n",
    "max_date = pd.to_datetime(df[\"InvoiceDate\"].max())\n",
    "cust[\"recency_days\"] = (max_date - pd.to_datetime(cust[\"last_date\"])).dt.total_seconds() / 86400.0\n",
    "cust = cust.drop(columns=[\"last_date\"])\n",
    "\n",
    "# Mean basket size per customer\n",
    "basket_mean = (\n",
    "    basket_per_invoice.groupby(\"CustomerID\", observed=True)[\"items_per_invoice\"]\n",
    "      .mean().reset_index().rename(columns={\"CustomerID\":\"customerid\",\"items_per_invoice\":\"basket_size_mean\"})\n",
    ")\n",
    "\n",
    "# Assemble feature table\n",
    "feat = cust.merge(basket_mean, on=\"customerid\", how=\"left\")\n",
    "feat[\"basket_size_mean\"] = feat[\"basket_size_mean\"].fillna(0.0)\n",
    "\n",
    "# RFM-style score (invert recency because lower is better)\n",
    "r_rank = feat[\"recency_days\"].rank(ascending=False, pct=True)\n",
    "f_rank = feat[\"tx_count\"].rank(ascending=True,  pct=True)\n",
    "m_rank = feat[\"spend_sum\"].rank(ascending=True,  pct=True)\n",
    "feat[\"RFM_Score\"] = (1 - r_rank)*0.34 + f_rank*0.33 + m_rank*0.33\n",
    "\n",
    "# Tidy numerics\n",
    "num_cols = [\"tx_count\",\"spend_sum\",\"item_qty_sum\",\"basket_size_mean\",\"recency_days\",\"RFM_Score\"]\n",
    "feat[num_cols] = feat[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "feat = feat.dropna(subset=[\"tx_count\",\"spend_sum\",\"recency_days\"]).reset_index(drop=True)\n",
    "\n",
    "# Quick summary\n",
    "print(f\"Rows after cleaning: {len(df):,}\")\n",
    "print(f\"Unique customers: {df['CustomerID'].nunique():,}\")\n",
    "print(f\"Date range: {df['InvoiceDate'].min()} → {df['InvoiceDate'].max()}\")\n",
    "print(f\"Features table: {len(feat):,} customers, columns = {list(feat.columns)}\")\n",
    "\n",
    "# Aliases used later in the notebook\n",
    "features = feat.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b960646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context figures for slides/poster (uses df and features from previous cells)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "feat = features  # alias for this plotting cell\n",
    "\n",
    "FIGS = Path(CFG[\"paths\"][\"figures_dir\"]) if \"CFG\" in globals() else Path(\"reports/figures\")\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def savefig(name, dpi=160, tight=True):\n",
    "    if tight: plt.tight_layout()\n",
    "    plt.savefig(FIGS / name, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Daily transactions (seasonality)\n",
    "daily = df.set_index(\"InvoiceDate\").resample(\"D\").size()\n",
    "plt.figure(figsize=(8,3))\n",
    "daily.plot()\n",
    "plt.title(\"Daily transaction counts\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"# Transactions\")\n",
    "savefig(\"daily_tx.png\")\n",
    "\n",
    "# Top 15 SKUs by line count\n",
    "(df[\"StockCode\"].astype(str).value_counts().head(15)).plot(kind=\"bar\", figsize=(8,3))\n",
    "plt.title(\"Top 15 SKUs by line count\")\n",
    "plt.xlabel(\"SKU\"); plt.ylabel(\"Lines\")\n",
    "savefig(\"top_skus.png\")\n",
    "\n",
    "# Heavy-tail histograms (clipped at 99th percentile)\n",
    "core = feat.copy()\n",
    "clip = {\n",
    "    \"spend_sum\":        core[\"spend_sum\"].quantile(.99),\n",
    "    \"item_qty_sum\":     core[\"item_qty_sum\"].quantile(.99),\n",
    "    \"basket_size_mean\": core[\"basket_size_mean\"].quantile(.99),\n",
    "}\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,3.5))\n",
    "for i,(col,q) in enumerate(clip.items()):\n",
    "    ax[i].hist(core[col].clip(upper=q), bins=40)\n",
    "    ax[i].set_title(col)\n",
    "fig.suptitle(\"Core distributions (clipped for readability)\")\n",
    "savefig(\"hists_core.png\")\n",
    "\n",
    "# Price × Quantity hexbin (log scale for counts)\n",
    "from matplotlib.colors import LogNorm\n",
    "p = df[\"Price\"].clip(lower=0, upper=df[\"Price\"].quantile(0.99))\n",
    "q = df[\"Quantity\"].clip(lower=0, upper=df[\"Quantity\"].quantile(0.99))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 6))\n",
    "hb = ax.hexbin(p, q, gridsize=60, mincnt=1, norm=LogNorm())\n",
    "ax.set_xlabel(\"Unit Price\"); ax.set_ylabel(\"Quantity\")\n",
    "ax.set_title(\"Price vs Quantity (clipped, log scale)\")\n",
    "cb = fig.colorbar(hb, ax=ax); cb.set_label(\"Count (log scale)\")\n",
    "savefig(\"price_qty_hex.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Pareto: cumulative customers vs cumulative spend\n",
    "s = feat.sort_values(\"spend_sum\", ascending=False)[\"spend_sum\"].to_numpy()\n",
    "cum = s.cumsum()/s.sum()\n",
    "x = (np.arange(len(s))+1)/len(s)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(x, cum)\n",
    "plt.axvline(0.2, color=\"k\", ls=\"--\")\n",
    "plt.axhline(cum[int(0.2*len(s))], color=\"k\", ls=\"--\")\n",
    "plt.title(\"Pareto: cumulative customers vs cumulative spend\")\n",
    "plt.xlabel(\"Customer share\"); plt.ylabel(\"Spend share\")\n",
    "savefig(\"pareto_spend.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presenting variant: features silhouette: 0.8824713663751557\n",
      "Baseline K-means (features) → best k = 2 sil= 0.8824713663751557\n"
     ]
    }
   ],
   "source": [
    "# Baseline Clustering — KMeans Sweep on Features (Comparative Metrics)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = [c for c in features.columns if features[c].dtype.kind in \"if\" and c.lower()!=\"customerid\"]\n",
    "X_feat = features[num_cols].fillna(0).to_numpy()\n",
    "Xs_feat = StandardScaler().fit_transform(X_feat)\n",
    "\n",
    "def sweep_metrics(Xs, title=\"features\"):\n",
    "    ks, sil, db, ch = [], [], [], []\n",
    "    for k in range(CFG[\"clustering\"][\"kmeans\"][\"kmin\"], CFG[\"clustering\"][\"kmeans\"][\"kmax\"]+1):\n",
    "        km = KMeans(n_clusters=k, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], random_state=CFG[\"random_seed\"]).fit(Xs)\n",
    "        y = km.labels_\n",
    "        ks.append(k)\n",
    "        sil.append(silhouette_score(Xs, y))\n",
    "        db.append(davies_bouldin_score(Xs, y))\n",
    "        ch.append(calinski_harabasz_score(Xs, y))\n",
    "    plt.figure(figsize=(7,3))\n",
    "    plt.plot(ks, sil, \"-o\", label=\"Silhouette (↑)\")\n",
    "    plt.plot(ks, db,  \"-o\", label=\"Davies–Bouldin (↓)\")\n",
    "    plt.plot(ks, ch,  \"-o\", label=\"Calinski–Harabasz (↑)\")\n",
    "    plt.title(f\"Clustering metrics vs k — {title}\")\n",
    "    plt.xlabel(\"k\"); plt.legend()\n",
    "    savefig(f\"silhouette_k_{title}.png\")\n",
    "    best_k = ks[int(np.argmax(sil))]\n",
    "    return best_k, pd.DataFrame({\"k\":ks, \"silhouette\":sil, \"davies_bouldin\":db, \"calinski_harabasz\":ch})\n",
    "\n",
    "best_k_feat, met_feat = sweep_metrics(Xs_feat, \"features\")\n",
    "\n",
    "# Fit final baseline and export labels\n",
    "km_feat = KMeans(n_clusters=best_k_feat, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"],\n",
    "                 random_state=CFG[\"random_seed\"]).fit(Xs_feat)\n",
    "labels_feat = pd.DataFrame({\"customerid\": features[\"customerid\"], \"label\": km_feat.labels_})\n",
    "labels_feat.to_csv(PROC / \"kmeans_labels.csv\", index=False)\n",
    "\n",
    "# Profile heatmap (legible differences)\n",
    "def profile_heatmap(df_features, labels_df, outname, title):\n",
    "    df = df_features.merge(labels_df, on=\"customerid\", how=\"left\")\n",
    "    numc = [c for c in df.columns\n",
    "            if df[c].dtype.kind in \"if\" and c not in (\"customerid\", \"label\")]\n",
    "    prof = df.groupby(\"label\")[numc].mean()\n",
    "    profz = (prof - prof.mean())/prof.std(ddof=0)\n",
    "    plt.figure(figsize=(6,2.8))\n",
    "    plt.imshow(profz, aspect=\"auto\", cmap=\"coolwarm\")\n",
    "    plt.yticks(range(len(profz.index)), [f\"C{int(c)}\" for c in profz.index])\n",
    "    plt.xticks(range(len(numc)), numc, rotation=45, ha=\"right\")\n",
    "    plt.title(title); plt.colorbar(label=\"z-score\")\n",
    "    savefig(outname)\n",
    "\n",
    "profile_heatmap(features, labels_feat, \"cluster_profile_heatmap_features.png\", \"Profiles — features\")\n",
    "\n",
    "# Auto-select the variant to PRESENT on the poster\n",
    "scoreboard = []\n",
    "if 'met_feat' in globals():\n",
    "    scoreboard.append((\"features\", float(met_feat.loc[met_feat['k']==best_k_feat, 'silhouette'])))\n",
    "if 'met_ae' in globals():\n",
    "    scoreboard.append((\"ae\", float(met_ae.loc[met_ae['k']==best_k_ae, 'silhouette'])))\n",
    "if 'met_seq' in globals():\n",
    "    scoreboard.append((\"seq\", float(met_seq.loc[met_seq['k']==best_k_seq, 'silhouette'])))\n",
    "\n",
    "scoreboard = sorted(scoreboard, key=lambda t: -t[1])\n",
    "assert len(scoreboard) > 0, \"No clustering metrics available yet.\"\n",
    "PRES_VARIANT = scoreboard[0][0]\n",
    "print(\"Presenting variant:\", PRES_VARIANT, \"silhouette:\", scoreboard[0][1])\n",
    "\n",
    "# Dynamic label-frame map (only include what exists)\n",
    "variants = {}\n",
    "if 'labels_feat' in globals(): variants['features'] = labels_feat\n",
    "if 'labels_ae'   in globals(): variants['ae'] = labels_ae\n",
    "if 'labels_seq'  in globals(): variants['seq'] = labels_seq\n",
    "labels_df = variants[PRES_VARIANT]   # <-- use this for both plots below\n",
    "\n",
    "# Segment lift: spend per customer (ranked, annotated)\n",
    "Z = features.merge(labels_df, on=\"customerid\", how=\"inner\")\n",
    "\n",
    "overall = Z[\"spend_sum\"].mean()\n",
    "lift = (Z.groupby(\"label\", as_index=False)[\"spend_sum\"].mean()\n",
    "          .rename(columns={\"spend_sum\":\"mean_spend\"}))\n",
    "lift[\"lift\"] = lift[\"mean_spend\"] / overall\n",
    "lift = lift.sort_values(\"lift\", ascending=False, ignore_index=True)\n",
    "lift[\"segment\"] = [f\"S{i+1}\" for i in range(len(lift))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.2))\n",
    "ax.bar(lift[\"segment\"], lift[\"lift\"])\n",
    "ax.axhline(1.0, color=\"k\", ls=\"--\", lw=1)\n",
    "ax.set_ylabel(\"Spend per customer (× overall)\")\n",
    "ax.set_title(\"Segment lift: spend per customer\")\n",
    "for i, (_, r) in enumerate(lift.iterrows()):\n",
    "    ax.text(i, r[\"lift\"] + 0.03, f\"{r['lift']:.2f}×\", ha=\"center\", va=\"bottom\", fontsize=11)\n",
    "ax.set_ylim(0, max(1.1, lift[\"lift\"].max() * 1.15))\n",
    "ax.set_xlabel(\"Segment (ranked by lift)\")\n",
    "savefig(\"segment_lift_spend.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Re-engagement risk: share of “quiet” customers (≥ 75th pct recency)\n",
    "Z = features.merge(labels_df, on=\"customerid\", how=\"inner\")\n",
    "cut = np.nanpercentile(Z[\"recency_days\"], 75)\n",
    "Z[\"quiet\"] = (Z[\"recency_days\"] >= cut).astype(int)\n",
    "\n",
    "quiet_share = (Z.groupby(\"label\", as_index=False)[\"quiet\"].mean()\n",
    "                 .rename(columns={\"quiet\":\"share_quiet\"}))\n",
    "quiet_share = quiet_share.sort_values(\"share_quiet\", ascending=False, ignore_index=True)\n",
    "quiet_share[\"segment\"] = [f\"S{i+1}\" for i in range(len(quiet_share))]\n",
    "baseline = Z[\"quiet\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.2))\n",
    "ax.barh(quiet_share[\"segment\"], quiet_share[\"share_quiet\"])\n",
    "ax.axvline(baseline, color=\"k\", ls=\"--\", lw=1)\n",
    "ax.set_xlabel(\"Share quiet (≥ 75th %ile recency)\")\n",
    "ax.set_title(\"Re-engagement risk by segment\")\n",
    "for y, p in enumerate(quiet_share[\"share_quiet\"]):\n",
    "    ax.text(p + 0.01, y, f\"{p*100:.1f}%\", va=\"center\", fontsize=11)\n",
    "ax.set_xlim(0, max(quiet_share[\"share_quiet\"].max()*1.15, baseline*1.15))\n",
    "ax.invert_yaxis()  # S1 at top\n",
    "savefig(\"segment_reengage_share.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Export metrics\n",
    "met_feat.to_csv(PROC / \"metrics_features.csv\", index=False)\n",
    "print(\"Baseline K-means (features) → best k =\", best_k_feat,\n",
    "      \"sil=\", float(met_feat.loc[met_feat[\"k\"]==best_k_feat, \"silhouette\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\.venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "AE clustering → best k = 2   sil= 0.90397596\n"
     ]
    }
   ],
   "source": [
    "# Tabular Autoencoder (Embedding + Comparative Clustering)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Paths (in case this cell runs standalone)\n",
    "from pathlib import Path\n",
    "PROC = Path(CFG[\"paths\"][\"processed_dir\"]) if \"CFG\" in globals() else Path(\"data/processed\")\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_tab_ae(input_dim, bottleneck=8, l2=1e-4, p_drop=0.10):\n",
    "    reg = keras.regularizers.l2(l2)\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.BatchNormalization()(inp)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(p_drop)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    z = layers.Dense(bottleneck, activation=\"linear\", name=\"z\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", kernel_regularizer=reg)(z)\n",
    "    x = layers.Dropout(p_drop)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    out = layers.Dense(input_dim, activation=\"linear\")(x)\n",
    "    ae = keras.Model(inp, out)\n",
    "    enc = keras.Model(inp, z)\n",
    "    ae.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return ae, enc\n",
    "\n",
    "# Config shims / defaults\n",
    "ae_cfg   = (CFG.get(\"ae\", {}) if \"CFG\" in globals() else {})\n",
    "bottleneck = ae_cfg.get(\"bottleneck\", 8)\n",
    "l2         = ae_cfg.get(\"l2\", 1e-4)\n",
    "p_drop     = ae_cfg.get(\"dropout\", 0.10)      # map 'dropout' -> p_drop\n",
    "patience   = ae_cfg.get(\"patience\", 8)\n",
    "epochs     = ae_cfg.get(\"epochs\", 50)\n",
    "batch      = ae_cfg.get(\"batch\", 256)\n",
    "\n",
    "# Xs_feat must exist from your scaling cell\n",
    "Xs = Xs_feat\n",
    "\n",
    "ae, enc = build_tab_ae(Xs.shape[1], bottleneck=bottleneck, l2=l2, p_drop=p_drop)\n",
    "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True)]\n",
    "hist = ae.fit(Xs, Xs, validation_split=0.2, epochs=epochs, batch_size=batch, verbose=0, callbacks=cb)\n",
    "\n",
    "# Loss curve\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend(); plt.title(\"AE training (MSE)\")\n",
    "savefig(\"ae_loss.png\")\n",
    "\n",
    "# Embeddings -> cluster -> metrics\n",
    "E = enc.predict(Xs, verbose=0)\n",
    "best_k_ae, met_ae = sweep_metrics(E, \"ae\")    # Assumes your sweep_metrics() is defined earlier\n",
    "\n",
    "km_ae = KMeans(n_clusters=best_k_ae, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], random_state=CFG[\"random_seed\"]).fit(E)\n",
    "labels_ae = pd.DataFrame({\"customerid\": features[\"customerid\"], \"label\": km_ae.labels_})\n",
    "labels_ae.to_csv(PROC / \"embed_kmeans_labels.csv\", index=False)\n",
    "\n",
    "# Profile heatmap for slides (assumes profile_heatmap() defined earlier)\n",
    "profile_heatmap(features, labels_ae, \"cluster_profile_heatmap_ae.png\", \"Profiles — AE\")\n",
    "\n",
    "met_ae.to_csv(PROC / \"metrics_ae.csv\", index=False)\n",
    "print(\"AE clustering → best k =\", best_k_ae, \"  sil=\",\n",
    "      met_ae.loc[met_ae[\"k\"]==best_k_ae, \"silhouette\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b2dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "SEQ clustering → best k = 2 sil= 0.08792641\n"
     ]
    }
   ],
   "source": [
    "# Sequence Encoder (LSTM Next-Token) + Comparative Clustering (habit vs variety)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Helpers / Shims \n",
    "def _rename_first_match(df, std, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df if c == std else df.rename(columns={c: std})\n",
    "    raise KeyError(f\"Required column '{std}' not found. Looked for: {candidates}. \"\n",
    "                   f\"Available (head): {list(df.columns)[:15]}\")\n",
    "\n",
    "# Use cleaned dataframe\n",
    "if \"df\" not in globals():\n",
    "    raise NameError(\"Missing `df`. Run the loading/cleaning + feature engineering cells first.\")\n",
    "\n",
    "# Normalize the few names we need here (safe no-ops if already standard)\n",
    "df = df.copy()\n",
    "df = _rename_first_match(df, \"CustomerID\",  [\"CustomerID\", \"Customer ID\", \"customerid\"])\n",
    "df = _rename_first_match(df, \"InvoiceDate\", [\"InvoiceDate\", \"Invoice Date\", \"date\"])\n",
    "df = _rename_first_match(df, \"StockCode\",   [\"StockCode\", \"Stock Code\", \"SKU\", \"ProductCode\", \"Product Code\", \"Description\"])\n",
    "\n",
    "# Strong types we rely on\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"CustomerID\", \"InvoiceDate\"]).copy()\n",
    "df[\"CustomerID\"] = pd.to_numeric(df[\"CustomerID\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "df = df.dropna(subset=[\"CustomerID\"]).astype({\"CustomerID\":\"int64\"})\n",
    "df[\"StockCode\"] = df[\"StockCode\"].astype(str).str.strip()\n",
    "\n",
    "# Build per-customer SKU sequences in time order\n",
    "cust_seq = (df.sort_values([\"CustomerID\",\"InvoiceDate\"])\n",
    "              .groupby(\"CustomerID\", observed=True)[\"StockCode\"]\n",
    "              .apply(list))\n",
    "\n",
    "# Vocabulary: keep frequent tokens\n",
    "seq_cfg   = CFG.get(\"seq\", {}) if \"CFG\" in globals() else {}\n",
    "min_count = seq_cfg.get(\"min_count\", 5)\n",
    "max_vocab = seq_cfg.get(\"max_vocab\", 2000)\n",
    "seq_len   = seq_cfg.get(\"seq_len\", 50)\n",
    "batch     = seq_cfg.get(\"batch\", 256)\n",
    "epochs    = seq_cfg.get(\"epochs\", 10)\n",
    "\n",
    "cnt  = Counter([t for seq in cust_seq for t in seq])\n",
    "keep = [t for t,c in cnt.items() if c >= min_count]\n",
    "keep = [t for t,_ in sorted(((t,cnt[t]) for t in keep), key=lambda z: -z[1])][:max_vocab]\n",
    "tok2id = {t:i+2 for i,t in enumerate(keep)}   # 0=PAD, 1=UNK\n",
    "PAD, UNK = 0, 1\n",
    "\n",
    "def encode(seq): return [tok2id.get(t, UNK) for t in seq]\n",
    "\n",
    "# Sliding windows for next-token task\n",
    "X_tok, y_tok = [], []\n",
    "for seq in cust_seq:\n",
    "    ids = encode(seq)\n",
    "    if len(ids) < 2: \n",
    "        continue\n",
    "    for i in range(1, len(ids)):\n",
    "        window = ids[max(0, i-seq_len):i]\n",
    "        if len(window) < 2:\n",
    "            continue\n",
    "        X_tok.append(window[-seq_len:])\n",
    "        y_tok.append(ids[i])\n",
    "\n",
    "if len(X_tok) == 0:\n",
    "    raise RuntimeError(\"No sequences long enough to build training samples. \"\n",
    "                       \"Try lowering seq_len or min_count in CFG['seq'].\")\n",
    "\n",
    "X_tok = pad_sequences(X_tok, maxlen=seq_len, padding=\"pre\", truncating=\"pre\", value=PAD)\n",
    "y_tok  = np.array(y_tok, dtype=\"int32\")\n",
    "\n",
    "# Optional cap to keep runtime/memory in check\n",
    "max_samples = seq_cfg.get(\"max_samples\")\n",
    "if max_samples and len(X_tok) > max_samples:\n",
    "    rng = np.random.default_rng(1337)\n",
    "    take = rng.choice(len(X_tok), size=max_samples, replace=False)\n",
    "    X_tok, y_tok = X_tok[take], y_tok[take]\n",
    "\n",
    "# Train/val split\n",
    "n   = len(X_tok)\n",
    "idx = np.arange(n); np.random.shuffle(idx)\n",
    "cut = int(0.8*n)\n",
    "tr, va = idx[:cut], idx[cut:]\n",
    "Xtr, Xva, ytr, yva = X_tok[tr], X_tok[va], y_tok[tr], y_tok[va]\n",
    "\n",
    "# LSTM next-token classifier (penultimate layer as embedding)\n",
    "vocab_size = len(tok2id) + 2\n",
    "embed_dim  = 64\n",
    "lstm_dim   = 64\n",
    "\n",
    "inp = keras.Input(shape=(seq_len,), dtype=\"int32\")\n",
    "x   = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inp)\n",
    "x   = layers.SpatialDropout1D(0.1)(x)\n",
    "x   = layers.LSTM(lstm_dim, return_sequences=False)(x)\n",
    "z   = layers.Dropout(0.2, name=\"seq_embedding\")(x)\n",
    "out = layers.Dense(vocab_size, activation=\"softmax\")(z)\n",
    "\n",
    "seq_model = keras.Model(inp, out)\n",
    "seq_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "cb2 = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)]\n",
    "\n",
    "hist_seq = seq_model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "                         epochs=epochs, batch_size=batch, verbose=0, callbacks=cb2)\n",
    "\n",
    "# Training curves (for slides/poster)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(hist_seq.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist_seq.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend(); plt.title(\"LSTM training (next-token)\")\n",
    "savefig(\"lstm_loss.png\")\n",
    "\n",
    "# Per-customer embeddings: embed last seq_len tokens per customer\n",
    "enc_seq = keras.Model(seq_model.input, z)\n",
    "\n",
    "cust_ids, cust_emb = [], []\n",
    "for cid, seq in cust_seq.items():\n",
    "    ids = encode(seq)\n",
    "    if not ids:\n",
    "        continue\n",
    "    x = pad_sequences([ids[-seq_len:]], maxlen=seq_len, padding=\"pre\", truncating=\"pre\", value=PAD)\n",
    "    e = enc_seq.predict(x, verbose=0)[0]\n",
    "    cust_ids.append(cid); cust_emb.append(e)\n",
    "\n",
    "seq_emb = pd.DataFrame(cust_emb, columns=[f\"e{i}\" for i in range(len(cust_emb[0]))])\n",
    "seq_emb.insert(0, \"customerid\", cust_ids)\n",
    "\n",
    "# Cluster sequence embeddings and record metrics for comparison\n",
    "E2 = seq_emb.drop(columns=[\"customerid\"]).to_numpy()\n",
    "best_k_seq, met_seq = sweep_metrics(E2, \"seq\")  # Uses earlier sweep_metrics\n",
    "\n",
    "km_seq = KMeans(\n",
    "    n_clusters=best_k_seq, \n",
    "    n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], \n",
    "    random_state=CFG[\"random_seed\"]\n",
    ").fit(E2)\n",
    "\n",
    "labels_seq = pd.DataFrame({\"customerid\": seq_emb[\"customerid\"], \"label\": km_seq.labels_})\n",
    "labels_seq.to_csv(Path(CFG[\"paths\"][\"processed_dir\"]) / \"embed_kmeans_labels_seq.csv\", index=False)\n",
    "\n",
    "profile_heatmap(features, labels_seq, \"cluster_profile_heatmap_seq.png\", \"Profiles — Sequence\")\n",
    "\n",
    "met_seq.to_csv(Path(CFG[\"paths\"][\"processed_dir\"]) / \"metrics_seq.csv\", index=False)\n",
    "print(\"SEQ clustering → best k =\", best_k_seq,\n",
    "      \"sil=\", met_seq.loc[met_seq[\"k\"]==best_k_seq, \"silhouette\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f986823",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xs_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m         vals\u001b[38;5;241m.\u001b[39mappend(silhouette_score(X, km\u001b[38;5;241m.\u001b[39mlabels_))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(vals)\n\u001b[1;32m----> 9\u001b[0m sv_feat \u001b[38;5;241m=\u001b[39m sil_vs_seed(\u001b[43mXs_feat\u001b[49m, k\u001b[38;5;241m=\u001b[39mbest_k_feat)\n\u001b[0;32m     10\u001b[0m sv_ae   \u001b[38;5;241m=\u001b[39m sil_vs_seed(E,        k\u001b[38;5;241m=\u001b[39mbest_k_ae)\n\u001b[0;32m     11\u001b[0m sv_seq  \u001b[38;5;241m=\u001b[39m sil_vs_seed(StandardScaler()\u001b[38;5;241m.\u001b[39mfit_transform(E2), k\u001b[38;5;241m=\u001b[39mbest_k_seq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xs_feat' is not defined"
     ]
    }
   ],
   "source": [
    "# Stability vs seed (boxplots of silhouette scores across variants)\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# assume you already computed arrays sil_feat, sil_ae, sil_seq\n",
    "labels = [\"features\", \"ae\", \"seq\"]\n",
    "data   = [sil_feat, sil_ae, sil_seq]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.6, 3.4))\n",
    "bp = ax.boxplot(data, patch_artist=True, labels=labels, widths=0.55,\n",
    "                boxprops=dict(facecolor=\"#3B82F6\", alpha=0.35, linewidth=1.2),\n",
    "                medianprops=dict(color=\"#1F2937\", linewidth=1.5),\n",
    "                whiskerprops=dict(color=\"#4B5563\"),\n",
    "                capprops=dict(color=\"#4B5563\"),\n",
    "                flierprops=dict(marker=\".\", markersize=3, markerfacecolor=\"#3B82F6\", alpha=0.5))\n",
    "ax.set_ylabel(\"Silhouette\")\n",
    "ax.set_title(\"Stability vs seed\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS / \"stability_box.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a343faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mttng\\AppData\\Local\\Temp\\ipykernel_15600\\2899549644.py:18: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  if tight: plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "# Anomaly Detection figs (AE log-MSE + IF×AE hexbin)\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogFormatter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "try:\n",
    "    FIGS\n",
    "except NameError:\n",
    "    FIGS = Path(\"reports/figures\"); FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    savefig\n",
    "except NameError:\n",
    "    def savefig(name, dpi=300, tight=True):\n",
    "        if tight: plt.tight_layout()\n",
    "        plt.savefig(FIGS / name, dpi=dpi, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# If 'scores' isn't in memory, try to load it.\n",
    "if \"scores\" not in globals():\n",
    "    cand = Path(\"data/processed/anomaly_scores.csv\")\n",
    "    if not cand.exists():\n",
    "        raise RuntimeError(\"Need a DataFrame 'scores' with columns ['if_score','ae_mse'].\")\n",
    "    scores = pd.read_csv(cand)\n",
    "\n",
    "\n",
    "# 1. Agreement plot (IF vs AE) — hexbin with % and labeled thresholds\n",
    "df_agree = scores.copy()\n",
    "df_agree[\"ae_log\"] = np.log10(np.clip(df_agree[\"ae_mse\"].values, np.finfo(float).eps, None))\n",
    "\n",
    "q = 0.97  # top 3% thresholds\n",
    "th_if  = np.quantile(df_agree[\"if_score\"], q)\n",
    "th_aeL = np.quantile(df_agree[\"ae_log\"],  q)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.4, 5.6))\n",
    "hb = ax.hexbin(\n",
    "    df_agree[\"if_score\"], df_agree[\"ae_log\"],\n",
    "    gridsize=45, mincnt=1, cmap=\"viridis\", norm=LogNorm()\n",
    ")\n",
    "cb = fig.colorbar(hb, ax=ax, format=LogFormatter(10))\n",
    "cb.set_label(\"Count (log scale)\")\n",
    "\n",
    "# Threshold lines + tiny labels\n",
    "ax.axvline(th_if,  color=\"k\", ls=\"--\", lw=1)\n",
    "ax.axhline(th_aeL, color=\"k\", ls=\"--\", lw=1)\n",
    "ax.text(th_if, ax.get_ylim()[1], \" IF q97 \", ha=\"left\", va=\"top\", fontsize=10, rotation=90,\n",
    "        bbox=dict(fc=\"white\", ec=\"0.6\", pad=0.2))\n",
    "ax.text(ax.get_xlim()[1], th_aeL, \" AE q97 \", ha=\"right\", va=\"bottom\", fontsize=10,\n",
    "        bbox=dict(fc=\"white\", ec=\"0.6\", pad=0.2))\n",
    "\n",
    "# Agreement count + percent in top-right quadrant\n",
    "mask_agree = (df_agree[\"if_score\"] >= th_if) & (df_agree[\"ae_log\"] >= th_aeL)\n",
    "n_agree = int(mask_agree.sum())\n",
    "pct_agree = 100.0 * n_agree / len(df_agree)\n",
    "ax.text(\n",
    "    0.02, 0.98,\n",
    "    f\"Top-right (agree) = {n_agree}  ({pct_agree:.1f}%)\",\n",
    "    transform=ax.transAxes, va=\"top\", ha=\"left\", fontsize=12,\n",
    "    bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"0.6\")\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Isolation Forest score\")\n",
    "ax.set_ylabel(\"log10(AE reconstruction MSE)\")\n",
    "ax.set_title(\"Anomaly agreement: IF vs AE\")\n",
    "savefig(\"anomaly_agreement_hex.png\")\n",
    "\n",
    "# 2. AE reconstruction error — log histogram with lower-right inset\n",
    "xlog = np.log10(np.clip(scores[\"ae_mse\"].values, np.finfo(float).eps, None))\n",
    "p95, p99 = np.percentile(xlog, [95, 99])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5.2))\n",
    "n, bins, _ = ax.hist(xlog, bins=60, color=\"#2B6CB0\", alpha=0.92,\n",
    "                     edgecolor=\"white\", linewidth=0.3)\n",
    "\n",
    "# 95th / 99th markers + light tail shading\n",
    "ax.axvline(p95, color=\"k\", ls=\"--\", lw=1)\n",
    "ax.axvline(p99, color=\"k\", ls=\":\",  lw=1)\n",
    "ax.text(p95, ax.get_ylim()[1]*0.94, \"95th %\", ha=\"right\", va=\"top\", fontsize=11)\n",
    "ax.text(p99, ax.get_ylim()[1]*0.88, \"99th %\", ha=\"right\", va=\"top\", fontsize=11)\n",
    "ax.fill_between(\n",
    "    bins, 0, np.interp(bins, (bins[:-1]+bins[1:])/2, n),\n",
    "    where=bins >= p99, color=\"#E53E3E\", alpha=0.22, step=\"pre\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"log10(AE reconstruction MSE)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"AE reconstruction error (log scale)\")\n",
    "\n",
    "# Inset: zoom the tail (lower-right, unobtrusive)\n",
    "axins = inset_axes(ax, width=\"34%\", height=\"58%\", loc=\"lower right\",\n",
    "                   bbox_to_anchor=(0.02, 0.05, 1, 1), bbox_transform=ax.transAxes, borderpad=0.6)\n",
    "axins.hist(xlog, bins=60, color=\"#2B6CB0\", alpha=0.92, edgecolor=\"white\", linewidth=0.3)\n",
    "axins.set_xlim(p95, xlog.max())\n",
    "axins.set_yticks([]); axins.set_xticks([])\n",
    "axins.set_title(\"Tail zoom\", fontsize=9)\n",
    "\n",
    "# Optional: a subtle connector line from inset to main\n",
    "con = ConnectionPatch(\n",
    "    xyA=(p99, 0), coordsA=ax.transData,\n",
    "    xyB=(axins.get_xlim()[0], 0), coordsB=axins.transData,\n",
    "    color=\"0.5\", lw=0.8, alpha=0.7\n",
    ")\n",
    "fig.add_artist(con)\n",
    "\n",
    "savefig(\"ae_mse_hist.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054802d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures ready: 20  /  20\n",
      "\n",
      "CSV/Parquet in: C:\\Users\\mttng\\Downloads\\retail-segmentation-anomalies\\data\\processed\n",
      "['anomaly_scores.csv', 'anomaly_top10.csv', 'embed_kmeans_labels.csv', 'embed_kmeans_labels_seq.csv', 'kmeans_labels.csv', 'metrics_ae.csv', 'metrics_features.csv', 'metrics_seq.csv']\n"
     ]
    }
   ],
   "source": [
    "# Exports For What to drop into slides/poster\n",
    "export_list = [\n",
    "    \"daily_tx.png\",\"top_skus.png\",\"hists_core.png\",\"price_qty_hex.png\",\"pareto_spend.png\",\n",
    "    \"silhouette_k_features.png\",\"cluster_profile_heatmap_features.png\",\"segment_lift_spend.png\",\"segment_reengage_share.png\",\n",
    "    \"ae_loss.png\",\"silhouette_k_ae.png\",\"cluster_profile_heatmap_ae.png\",\n",
    "    \"lstm_loss.png\",\"silhouette_k_seq.png\",\"cluster_profile_heatmap_seq.png\",\n",
    "    \"stability_box.png\",\n",
    "    \"if_score_hist.png\",\"ae_mse_hist.png\",\"anomaly_agreement_scatter.png\",\"anomaly_cume_spend.png\",\n",
    "]\n",
    "missing = [f for f in export_list if not (FIGS/f).exists()]\n",
    "print(\"Figures ready:\", len(export_list)-len(missing), \" / \", len(export_list))\n",
    "if missing:\n",
    "    print(\"Missing:\", missing)\n",
    "\n",
    "print(\"\\nCSV/Parquet in:\", PROC)\n",
    "print(sorted([p.name for p in PROC.glob(\"*.csv\")]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fbab18",
   "metadata": {},
   "source": [
    "# DSII — Retail Customer Segmentation & Anomaly Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bddb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and Directories\n",
    "from pathlib import Path\n",
    "BASE  = Path.cwd()\n",
    "DATA  = BASE / \"data\"\n",
    "PROC  = DATA / \"processed\"\n",
    "FIGS  = BASE / \"reports\" / \"figures\"\n",
    "\n",
    "for p in [DATA, PROC, FIGS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_CSV = BASE / \"online_retail_II.csv\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81759d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      3\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Setup Enviornment\n",
    "import os, random, json, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED); os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\n",
    "\n",
    "# Paths (relative to notebook)\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATASET_PATH = ROOT / \"online_retail_II.csv\"  # <-- keep CSV next to notebook\n",
    "PROC = ROOT / \"data\" / \"processed\"\n",
    "FIGS = ROOT / \"reports\" / \"figures\"\n",
    "PROC.mkdir(parents=True, exist_ok=True); FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"paths\": {\"processed_dir\": str(PROC), \"figures_dir\": str(FIGS)},\n",
    "    \"clustering\": {\"kmeans\": {\"kmin\": 2, \"kmax\": 10, \"n_init\": 10}},\n",
    "    \"projection\": {\"method\": \"umap\", \"random_state\": RANDOM_SEED},\n",
    "    \"seq\": {\"seq_len\": 50, \"max_vocab\": 2000, \"min_count\": 5, \"batch\": 256, \"epochs\": 10},\n",
    "    \"ae\": {\"bottleneck\": 8, \"l2\": 1e-4, \"dropout\": 0.10, \"epochs\": 200, \"batch\": 256, \"patience\": 10},\n",
    "}\n",
    "\n",
    "print(\"Config OK →\", json.dumps(CFG, indent=2))\n",
    "print(\"Data:\", DATASET_PATH)\n",
    "print(\"Outputs →\", PROC, \"and\", FIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d94b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility pathing for figures & timers\n",
    "def savefig(name, tight=True, dpi=200):\n",
    "    if tight: plt.tight_layout()\n",
    "    out = Path(CFG[\"paths\"][\"figures_dir\"]) / name\n",
    "    plt.savefig(out, dpi=dpi)\n",
    "    plt.close()\n",
    "    return out\n",
    "\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer(msg):\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    print(f\"[start] {msg}\")\n",
    "    yield\n",
    "    print(f\"[done]  {msg} in {time.time()-t0:.2f}s\")\n",
    "\n",
    "def ensure_cols(df):\n",
    "    # tolerant column mapper\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    def find(*cands):\n",
    "        for k in cands:\n",
    "            if k in cols: return cols[k]\n",
    "        return None\n",
    "    mapping = {\n",
    "        \"Invoice\": find(\"invoice\",\"invoiceno\",\"invoice no\",\"invoicenumber\"),\n",
    "        \"InvoiceDate\": find(\"invoicedate\",\"date\"),\n",
    "        \"CustomerID\": find(\"customer id\",\"customerid\",\"custid\",\"cust id\"),\n",
    "        \"StockCode\": find(\"stockcode\",\"sku\",\"productcode\",\"product code\"),\n",
    "        \"Quantity\": find(\"quantity\",\"qty\",\"count\"),\n",
    "        \"Price\": find(\"price\",\"unitprice\",\"unit price\"),\n",
    "        \"Description\": find(\"description\",\"desc\")\n",
    "    }\n",
    "    missing = [k for k,v in mapping.items() if v is None and k not in [\"Description\"]]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b808ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] Load CSV\n",
      "[done]  Load CSV in 0.94s\n",
      "Rows after cleaning: 805549\n",
      "Unique customers: 5878\n",
      "Date range: 2009-12-01 07:45:00 → 2011-12-09 12:50:00\n"
     ]
    }
   ],
   "source": [
    "# Loading & Cleaning Data\n",
    "with timer(\"Load CSV\"):\n",
    "    raw = pd.read_csv(DATASET_PATH, encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "colmap = ensure_cols(raw)\n",
    "raw = raw.rename(columns={v:k for k,v in colmap.items() if v is not None})\n",
    "\n",
    "# Parse dates, compute line totals\n",
    "raw[\"InvoiceDate\"] = pd.to_datetime(raw[\"InvoiceDate\"], errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[\"InvoiceDate\"])\n",
    "raw[\"LineTotal\"] = raw[\"Quantity\"] * raw[\"Price\"]\n",
    "\n",
    "# Remove cancellations/returns (Invoice starts with 'C'), invalid rows\n",
    "mask_cancel = raw[\"Invoice\"].astype(str).str.startswith(\"C\")\n",
    "raw = raw.loc[~mask_cancel].copy()\n",
    "raw = raw[(raw[\"Quantity\"] > 0) & (raw[\"Price\"] > 0)]\n",
    "raw = raw.dropna(subset=[\"CustomerID\"])\n",
    "raw[\"CustomerID\"] = raw[\"CustomerID\"].astype(str)\n",
    "\n",
    "print(\"Rows after cleaning:\", len(raw))\n",
    "print(\"Unique customers:\", raw[\"CustomerID\"].nunique())\n",
    "print(\"Date range:\", raw[\"InvoiceDate\"].min(), \"→\", raw[\"InvoiceDate\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b63a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 805,549\n",
      "Unique customers: 5,878\n",
      "Date range: 2009-12-01 07:45:00 → 2011-12-09 12:50:00\n",
      "Features table: 5,878 customers, columns = ['customerid', 'tx_count', 'spend_sum', 'item_qty_sum', 'recency_days', 'basket_size_mean', 'RFM_Score']\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering (RFM + interpretable features)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Load CSV\n",
    "CSV_PATH = Path(globals().get(\"DATASET_PATH\", \"online_retail_II.csv\"))\n",
    "raw = pd.read_csv(CSV_PATH, encoding_errors=\"ignore\")\n",
    "\n",
    "# 2. Harmonize column names across variants\n",
    "def rename_first_match(df, std, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df if c == std else df.rename(columns={c: std})\n",
    "    raise KeyError(f\"Required column '{std}' not found. Looked for {candidates}. Got {list(df.columns)[:20]} ...\")\n",
    "\n",
    "df = raw.copy()\n",
    "df = rename_first_match(df, \"CustomerID\",  [\"CustomerID\",\"Customer ID\",\"customerid\"])\n",
    "df = rename_first_match(df, \"InvoiceNo\",   [\"InvoiceNo\",\"Invoice\",\"Invoice No\"])\n",
    "df = rename_first_match(df, \"InvoiceDate\", [\"InvoiceDate\",\"Invoice Date\",\"date\",\"Date\"])\n",
    "df = rename_first_match(df, \"Price\",       [\"Price\",\"UnitPrice\",\"Unit Price\"])\n",
    "df = rename_first_match(df, \"Quantity\",    [\"Quantity\",\"Qty\",\"quantity\"])\n",
    "df = rename_first_match(df, \"StockCode\",   [\"StockCode\",\"Stock Code\",\"SKU\",\"ProductCode\",\"Product Code\",\"Description\"])\n",
    "\n",
    "# 3. Strong typing & cleaning\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df[\"InvoiceNo\"]   = df[\"InvoiceNo\"].astype(str).str.strip()\n",
    "df[\"CustomerID\"]  = pd.to_numeric(df[\"CustomerID\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "# Drop cancels/returns (InvoiceNo that start with 'C'), invalid rows, and nulls\n",
    "before = len(df)\n",
    "df = df[~df[\"InvoiceNo\"].str.startswith(\"C\", na=False)]\n",
    "df = df.dropna(subset=[\"CustomerID\",\"InvoiceDate\"]).copy()\n",
    "df = df[(df[\"Quantity\"] > 0) & (df[\"Price\"] > 0)].copy()\n",
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(\"int64\")\n",
    "\n",
    "# 4. Line total\n",
    "if \"LineTotal\" not in df.columns:\n",
    "    df[\"LineTotal\"] = df[\"Quantity\"] * df[\"Price\"]\n",
    "\n",
    "# 5. Per-invoice basket stats\n",
    "basket_per_invoice = (\n",
    "    df.groupby([\"InvoiceNo\",\"CustomerID\"], observed=True)[\"StockCode\"]\n",
    "      .count().reset_index(name=\"items_per_invoice\")\n",
    ")\n",
    "\n",
    "# 6. Customer-level features\n",
    "g = df.groupby(\"CustomerID\", observed=True)\n",
    "cust = (\n",
    "    g.agg(\n",
    "        tx_count     = (\"InvoiceNo\",\"nunique\"),\n",
    "        spend_sum    = (\"LineTotal\",\"sum\"),\n",
    "        item_qty_sum = (\"Quantity\",\"sum\"),\n",
    "        last_date    = (\"InvoiceDate\",\"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"CustomerID\":\"customerid\"})\n",
    ")\n",
    "\n",
    "# Recency in days\n",
    "max_date = pd.to_datetime(df[\"InvoiceDate\"].max())\n",
    "cust[\"recency_days\"] = (max_date - pd.to_datetime(cust[\"last_date\"])).dt.total_seconds() / 86400.0\n",
    "cust = cust.drop(columns=[\"last_date\"])\n",
    "\n",
    "# Mean basket size per customer\n",
    "basket_mean = (\n",
    "    basket_per_invoice.groupby(\"CustomerID\", observed=True)[\"items_per_invoice\"]\n",
    "      .mean().reset_index().rename(columns={\"CustomerID\":\"customerid\",\"items_per_invoice\":\"basket_size_mean\"})\n",
    ")\n",
    "\n",
    "# Assemble feature table\n",
    "feat = cust.merge(basket_mean, on=\"customerid\", how=\"left\")\n",
    "feat[\"basket_size_mean\"] = feat[\"basket_size_mean\"].fillna(0.0)\n",
    "\n",
    "# RFM-style score (invert recency because lower is better)\n",
    "r_rank = feat[\"recency_days\"].rank(ascending=False, pct=True)\n",
    "f_rank = feat[\"tx_count\"].rank(ascending=True,  pct=True)\n",
    "m_rank = feat[\"spend_sum\"].rank(ascending=True,  pct=True)\n",
    "feat[\"RFM_Score\"] = (1 - r_rank)*0.34 + f_rank*0.33 + m_rank*0.33\n",
    "\n",
    "# Tidy numerics\n",
    "num_cols = [\"tx_count\",\"spend_sum\",\"item_qty_sum\",\"basket_size_mean\",\"recency_days\",\"RFM_Score\"]\n",
    "feat[num_cols] = feat[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "feat = feat.dropna(subset=[\"tx_count\",\"spend_sum\",\"recency_days\"]).reset_index(drop=True)\n",
    "\n",
    "# Quick summary\n",
    "print(f\"Rows after cleaning: {len(df):,}\")\n",
    "print(f\"Unique customers: {df['CustomerID'].nunique():,}\")\n",
    "print(f\"Date range: {df['InvoiceDate'].min()} → {df['InvoiceDate'].max()}\")\n",
    "print(f\"Features table: {len(feat):,} customers, columns = {list(feat.columns)}\")\n",
    "\n",
    "# Aliases used later in the notebook\n",
    "features = feat.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b960646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context figures for slides/poster (uses df and feat from the previous cell)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "FIGS = Path(CFG[\"paths\"][\"figures_dir\"]) if \"CFG\" in globals() else Path(\"reports/figures\")\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def savefig(name, dpi=160, tight=True):\n",
    "    if tight: plt.tight_layout()\n",
    "    plt.savefig(FIGS / name, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Daily transactions (seasonality)\n",
    "daily = df.set_index(\"InvoiceDate\").resample(\"D\").size()\n",
    "plt.figure(figsize=(8,3))\n",
    "daily.plot()\n",
    "plt.title(\"Daily transaction counts\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"# Transactions\")\n",
    "savefig(\"daily_tx.png\")\n",
    "\n",
    "# Top 15 SKUs by line count\n",
    "(df[\"StockCode\"].astype(str).value_counts().head(15)).plot(kind=\"bar\", figsize=(8,3))\n",
    "plt.title(\"Top 15 SKUs by line count\")\n",
    "plt.xlabel(\"SKU\"); plt.ylabel(\"Lines\")\n",
    "savefig(\"top_skus.png\")\n",
    "\n",
    "# Heavy-tail histograms (clipped at 99th percentile for readability)\n",
    "core = feat.copy()\n",
    "clip = {\n",
    "    \"spend_sum\":        core[\"spend_sum\"].quantile(.99),\n",
    "    \"item_qty_sum\":     core[\"item_qty_sum\"].quantile(.99),\n",
    "    \"basket_size_mean\": core[\"basket_size_mean\"].quantile(.99),\n",
    "}\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,3.5))\n",
    "for i,(col,q) in enumerate(clip.items()):\n",
    "    ax[i].hist(core[col].clip(upper=q), bins=40)\n",
    "    ax[i].set_title(col)\n",
    "fig.suptitle(\"Core distributions (clipped for readability)\")\n",
    "savefig(\"hists_core.png\")\n",
    "\n",
    "# Price × Quantity Hexbin (outlier intuition)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.hexbin(\n",
    "    df[\"Price\"].clip(lower=0, upper=df[\"Price\"].quantile(.99)),\n",
    "    df[\"Quantity\"].clip(lower=0, upper=df[\"Quantity\"].quantile(.99)),\n",
    "    gridsize=40\n",
    ")\n",
    "plt.xlabel(\"Unit Price\"); plt.ylabel(\"Quantity\"); plt.title(\"Price vs Quantity (clipped)\")\n",
    "savefig(\"price_qty_hex.png\")\n",
    "\n",
    "# Pareto curve: cumulative customers vs cumulative spend\n",
    "s = feat.sort_values(\"spend_sum\", ascending=False)[\"spend_sum\"].to_numpy()\n",
    "cum = s.cumsum()/s.sum()\n",
    "x = (np.arange(len(s))+1)/len(s)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(x, cum)\n",
    "plt.axvline(0.2, color=\"k\", ls=\"--\")\n",
    "plt.axhline(cum[int(0.2*len(s))], color=\"k\", ls=\"--\")\n",
    "plt.title(\"Pareto: cumulative customers vs cumulative spend\")\n",
    "plt.xlabel(\"Customer share\"); plt.ylabel(\"Spend share\")\n",
    "savefig(\"pareto_spend.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline K-means (features) → best k = 2 sil= 0.8824713663751557\n"
     ]
    }
   ],
   "source": [
    "# Baseline Clustering — KMeans Sweep on Features (Comparative Metrics) \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "num_cols = [c for c in features.columns if features[c].dtype.kind in \"if\" and c.lower()!=\"customerid\"]\n",
    "X_feat = features[num_cols].fillna(0).to_numpy()\n",
    "Xs_feat = StandardScaler().fit_transform(X_feat)\n",
    "\n",
    "def sweep_metrics(Xs, title=\"features\"):\n",
    "    ks, sil, db, ch = [], [], [], []\n",
    "    for k in range(CFG[\"clustering\"][\"kmeans\"][\"kmin\"], CFG[\"clustering\"][\"kmeans\"][\"kmax\"]+1):\n",
    "        km = KMeans(n_clusters=k, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], random_state=CFG[\"random_seed\"]).fit(Xs)\n",
    "        y = km.labels_\n",
    "        ks.append(k)\n",
    "        sil.append(silhouette_score(Xs, y))\n",
    "        db.append(davies_bouldin_score(Xs, y))\n",
    "        ch.append(calinski_harabasz_score(Xs, y))\n",
    "    plt.figure(figsize=(7,3))\n",
    "    plt.plot(ks, sil, \"-o\", label=\"Silhouette (↑)\")\n",
    "    plt.plot(ks, db,  \"-o\", label=\"Davies–Bouldin (↓)\")\n",
    "    plt.plot(ks, ch,  \"-o\", label=\"Calinski–Harabasz (↑)\")\n",
    "    plt.title(f\"Clustering metrics vs k — {title}\")\n",
    "    plt.xlabel(\"k\"); plt.legend()\n",
    "    savefig(f\"silhouette_k_{title}.png\")\n",
    "    best_k = ks[int(np.argmax(sil))]\n",
    "    return best_k, pd.DataFrame({\"k\":ks, \"silhouette\":sil, \"davies_bouldin\":db, \"calinski_harabasz\":ch})\n",
    "\n",
    "best_k_feat, met_feat = sweep_metrics(Xs_feat, \"features\")\n",
    "\n",
    "# Fit final baseline and export labels\n",
    "km_feat = KMeans(n_clusters=best_k_feat, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], random_state=CFG[\"random_seed\"]).fit(Xs_feat)\n",
    "labels_feat = pd.DataFrame({\"customerid\": features[\"customerid\"], \"label\": km_feat.labels_})\n",
    "labels_feat.to_csv(PROC / \"kmeans_labels.csv\", index=False)\n",
    "\n",
    "# Profile heatmap (legible differences)\n",
    "def profile_heatmap(df_features, labels_df, outname, title):\n",
    "    df = df_features.merge(labels_df, on=\"customerid\", how=\"left\")\n",
    "    numc = [c for c in df.columns if df[c].dtype.kind in \"if\" and c!=\"customerid\"]\n",
    "    prof = df.groupby(\"label\")[numc].mean()\n",
    "    profz = (prof - prof.mean())/prof.std(ddof=0)\n",
    "    plt.figure(figsize=(6,2.8))\n",
    "    plt.imshow(profz, aspect=\"auto\", cmap=\"coolwarm\")\n",
    "    plt.yticks(range(len(profz.index)), [f\"C{int(c)}\" for c in profz.index])\n",
    "    plt.xticks(range(len(numc)), numc, rotation=45, ha=\"right\")\n",
    "    plt.title(title); plt.colorbar(label=\"z-score\")\n",
    "    savefig(outname)\n",
    "\n",
    "profile_heatmap(features, labels_feat, \"cluster_profile_heatmap_features.png\", \"Profiles — features\")\n",
    "\n",
    "# Segment lift & re-engage share (actionability)\n",
    "df_feat = features.merge(labels_feat, on=\"customerid\")\n",
    "overall = df_feat[\"spend_sum\"].mean()\n",
    "lift = df_feat.groupby(\"label\")[\"spend_sum\"].mean() / overall\n",
    "lift.plot(kind=\"bar\", figsize=(4.5,3)); plt.axhline(1, color=\"k\", ls=\"--\")\n",
    "plt.ylabel(\"Relative to overall\"); plt.title(\"Segment lift: spend per customer (features)\")\n",
    "savefig(\"segment_lift_spend.png\")\n",
    "\n",
    "thr = features[\"recency_days\"].quantile(.75)  # \"gone quiet\" threshold\n",
    "share = df_feat.assign(quiet=df_feat[\"recency_days\"]>=thr).groupby(\"label\")[\"quiet\"].mean()\n",
    "share.plot(kind=\"bar\", figsize=(4.5,3)); plt.ylim(0,1)\n",
    "plt.ylabel(\"Share quiet (≥ 75th %ile recency)\"); plt.title(\"Re-engagement risk by segment (features)\")\n",
    "savefig(\"segment_reengage_share.png\")\n",
    "\n",
    "met_feat.to_csv(PROC / \"metrics_features.csv\", index=False)\n",
    "print(\"Baseline K-means (features) → best k =\", best_k_feat, \"sil=\", met_feat.loc[met_feat[\"k\"]==best_k_feat, \"silhouette\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE clustering → best k = 2   sil= 0.9239121675491333\n"
     ]
    }
   ],
   "source": [
    "# Tabular Autoencoder (Embedding + Comparative Clustering)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Paths (in case this cell runs standalone)\n",
    "from pathlib import Path\n",
    "PROC = Path(CFG[\"paths\"][\"processed_dir\"]) if \"CFG\" in globals() else Path(\"data/processed\")\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_tab_ae(input_dim, bottleneck=8, l2=1e-4, p_drop=0.10):\n",
    "    reg = keras.regularizers.l2(l2)\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.BatchNormalization()(inp)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    x = layers.Dropout(p_drop)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    z = layers.Dense(bottleneck, activation=\"linear\", name=\"z\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", kernel_regularizer=reg)(z)\n",
    "    x = layers.Dropout(p_drop)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", kernel_regularizer=reg)(x)\n",
    "    out = layers.Dense(input_dim, activation=\"linear\")(x)\n",
    "    ae = keras.Model(inp, out)\n",
    "    enc = keras.Model(inp, z)\n",
    "    ae.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return ae, enc\n",
    "\n",
    "# Config shims / defaults\n",
    "ae_cfg   = (CFG.get(\"ae\", {}) if \"CFG\" in globals() else {})\n",
    "bottleneck = ae_cfg.get(\"bottleneck\", 8)\n",
    "l2         = ae_cfg.get(\"l2\", 1e-4)\n",
    "p_drop     = ae_cfg.get(\"dropout\", 0.10)      # map 'dropout' -> p_drop\n",
    "patience   = ae_cfg.get(\"patience\", 8)\n",
    "epochs     = ae_cfg.get(\"epochs\", 50)\n",
    "batch      = ae_cfg.get(\"batch\", 256)\n",
    "\n",
    "# Xs_feat must exist from your scaling cell\n",
    "Xs = Xs_feat\n",
    "\n",
    "ae, enc = build_tab_ae(Xs.shape[1], bottleneck=bottleneck, l2=l2, p_drop=p_drop)\n",
    "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True)]\n",
    "hist = ae.fit(Xs, Xs, validation_split=0.2, epochs=epochs, batch_size=batch, verbose=0, callbacks=cb)\n",
    "\n",
    "# Loss curve\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(hist.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend(); plt.title(\"AE training (MSE)\")\n",
    "savefig(\"ae_loss.png\")\n",
    "\n",
    "# Embeddings -> cluster -> metrics\n",
    "E = enc.predict(Xs, verbose=0)\n",
    "best_k_ae, met_ae = sweep_metrics(E, \"ae\")    # Assumes your sweep_metrics() is defined earlier\n",
    "\n",
    "km_ae = KMeans(n_clusters=best_k_ae, n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], random_state=CFG[\"random_seed\"]).fit(E)\n",
    "labels_ae = pd.DataFrame({\"customerid\": features[\"customerid\"], \"label\": km_ae.labels_})\n",
    "labels_ae.to_csv(PROC / \"embed_kmeans_labels.csv\", index=False)\n",
    "\n",
    "# Profile heatmap for slides (assumes profile_heatmap() defined earlier)\n",
    "profile_heatmap(features, labels_ae, \"cluster_profile_heatmap_ae.png\", \"Profiles — AE\")\n",
    "\n",
    "met_ae.to_csv(PROC / \"metrics_ae.csv\", index=False)\n",
    "print(\"AE clustering → best k =\", best_k_ae, \"  sil=\",\n",
    "      met_ae.loc[met_ae[\"k\"]==best_k_ae, \"silhouette\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\dsii-retail-segmentation-repo\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\mttng\\Downloads\\dsii-retail-segmentation-repo\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "SEQ clustering → best k = 2 sil= 0.0949288159608841\n"
     ]
    }
   ],
   "source": [
    "# Sequence Encoder (LSTM Next-Token) + Comparative Clustering (habit vs variety)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Helpers / Shims \n",
    "def _rename_first_match(df, std, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return df if c == std else df.rename(columns={c: std})\n",
    "    raise KeyError(f\"Required column '{std}' not found. Looked for: {candidates}. \"\n",
    "                   f\"Available (head): {list(df.columns)[:15]}\")\n",
    "\n",
    "# Use cleaned dataframe\n",
    "if \"df\" not in globals():\n",
    "    raise NameError(\"Missing `df`. Run the loading/cleaning + feature engineering cells first.\")\n",
    "\n",
    "# Normalize the few names we need here (safe no-ops if already standard)\n",
    "df = df.copy()\n",
    "df = _rename_first_match(df, \"CustomerID\",  [\"CustomerID\", \"Customer ID\", \"customerid\"])\n",
    "df = _rename_first_match(df, \"InvoiceDate\", [\"InvoiceDate\", \"Invoice Date\", \"date\"])\n",
    "df = _rename_first_match(df, \"StockCode\",   [\"StockCode\", \"Stock Code\", \"SKU\", \"ProductCode\", \"Product Code\", \"Description\"])\n",
    "\n",
    "# Strong types we rely on\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"CustomerID\", \"InvoiceDate\"]).copy()\n",
    "df[\"CustomerID\"] = pd.to_numeric(df[\"CustomerID\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "df = df.dropna(subset=[\"CustomerID\"]).astype({\"CustomerID\":\"int64\"})\n",
    "df[\"StockCode\"] = df[\"StockCode\"].astype(str).str.strip()\n",
    "\n",
    "# Build per-customer SKU sequences in time order\n",
    "cust_seq = (df.sort_values([\"CustomerID\",\"InvoiceDate\"])\n",
    "              .groupby(\"CustomerID\", observed=True)[\"StockCode\"]\n",
    "              .apply(list))\n",
    "\n",
    "# Vocabulary: keep frequent tokens\n",
    "seq_cfg   = CFG.get(\"seq\", {}) if \"CFG\" in globals() else {}\n",
    "min_count = seq_cfg.get(\"min_count\", 5)\n",
    "max_vocab = seq_cfg.get(\"max_vocab\", 2000)\n",
    "seq_len   = seq_cfg.get(\"seq_len\", 50)\n",
    "batch     = seq_cfg.get(\"batch\", 256)\n",
    "epochs    = seq_cfg.get(\"epochs\", 10)\n",
    "\n",
    "cnt  = Counter([t for seq in cust_seq for t in seq])\n",
    "keep = [t for t,c in cnt.items() if c >= min_count]\n",
    "keep = [t for t,_ in sorted(((t,cnt[t]) for t in keep), key=lambda z: -z[1])][:max_vocab]\n",
    "tok2id = {t:i+2 for i,t in enumerate(keep)}   # 0=PAD, 1=UNK\n",
    "PAD, UNK = 0, 1\n",
    "\n",
    "def encode(seq): return [tok2id.get(t, UNK) for t in seq]\n",
    "\n",
    "# Sliding windows for next-token task\n",
    "X_tok, y_tok = [], []\n",
    "for seq in cust_seq:\n",
    "    ids = encode(seq)\n",
    "    if len(ids) < 2: \n",
    "        continue\n",
    "    for i in range(1, len(ids)):\n",
    "        window = ids[max(0, i-seq_len):i]\n",
    "        if len(window) < 2:\n",
    "            continue\n",
    "        X_tok.append(window[-seq_len:])\n",
    "        y_tok.append(ids[i])\n",
    "\n",
    "if len(X_tok) == 0:\n",
    "    raise RuntimeError(\"No sequences long enough to build training samples. \"\n",
    "                       \"Try lowering seq_len or min_count in CFG['seq'].\")\n",
    "\n",
    "X_tok = pad_sequences(X_tok, maxlen=seq_len, padding=\"pre\", truncating=\"pre\", value=PAD)\n",
    "y_tok  = np.array(y_tok, dtype=\"int32\")\n",
    "\n",
    "# Optional cap to keep runtime/memory in check\n",
    "max_samples = seq_cfg.get(\"max_samples\")\n",
    "if max_samples and len(X_tok) > max_samples:\n",
    "    rng = np.random.default_rng(1337)\n",
    "    take = rng.choice(len(X_tok), size=max_samples, replace=False)\n",
    "    X_tok, y_tok = X_tok[take], y_tok[take]\n",
    "\n",
    "# Train/val split\n",
    "n   = len(X_tok)\n",
    "idx = np.arange(n); np.random.shuffle(idx)\n",
    "cut = int(0.8*n)\n",
    "tr, va = idx[:cut], idx[cut:]\n",
    "Xtr, Xva, ytr, yva = X_tok[tr], X_tok[va], y_tok[tr], y_tok[va]\n",
    "\n",
    "# LSTM next-token classifier (penultimate layer as embedding)\n",
    "vocab_size = len(tok2id) + 2\n",
    "embed_dim  = 64\n",
    "lstm_dim   = 64\n",
    "\n",
    "inp = keras.Input(shape=(seq_len,), dtype=\"int32\")\n",
    "x   = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(inp)\n",
    "x   = layers.SpatialDropout1D(0.1)(x)\n",
    "x   = layers.LSTM(lstm_dim, return_sequences=False)(x)\n",
    "z   = layers.Dropout(0.2, name=\"seq_embedding\")(x)\n",
    "out = layers.Dense(vocab_size, activation=\"softmax\")(z)\n",
    "\n",
    "seq_model = keras.Model(inp, out)\n",
    "seq_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "cb2 = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)]\n",
    "\n",
    "hist_seq = seq_model.fit(Xtr, ytr, validation_data=(Xva, yva),\n",
    "                         epochs=epochs, batch_size=batch, verbose=0, callbacks=cb2)\n",
    "\n",
    "# Training curves (for slides/poster)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(hist_seq.history[\"loss\"], label=\"train\")\n",
    "plt.plot(hist_seq.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend(); plt.title(\"LSTM training (next-token)\")\n",
    "savefig(\"lstm_loss.png\")\n",
    "\n",
    "# Per-customer embeddings: embed last seq_len tokens per customer\n",
    "enc_seq = keras.Model(seq_model.input, z)\n",
    "\n",
    "cust_ids, cust_emb = [], []\n",
    "for cid, seq in cust_seq.items():\n",
    "    ids = encode(seq)\n",
    "    if not ids:\n",
    "        continue\n",
    "    x = pad_sequences([ids[-seq_len:]], maxlen=seq_len, padding=\"pre\", truncating=\"pre\", value=PAD)\n",
    "    e = enc_seq.predict(x, verbose=0)[0]\n",
    "    cust_ids.append(cid); cust_emb.append(e)\n",
    "\n",
    "seq_emb = pd.DataFrame(cust_emb, columns=[f\"e{i}\" for i in range(len(cust_emb[0]))])\n",
    "seq_emb.insert(0, \"customerid\", cust_ids)\n",
    "\n",
    "# Cluster sequence embeddings and record metrics for comparison\n",
    "E2 = seq_emb.drop(columns=[\"customerid\"]).to_numpy()\n",
    "best_k_seq, met_seq = sweep_metrics(E2, \"seq\")  # Uses earlier sweep_metrics\n",
    "\n",
    "km_seq = KMeans(\n",
    "    n_clusters=best_k_seq, \n",
    "    n_init=CFG[\"clustering\"][\"kmeans\"][\"n_init\"], \n",
    "    random_state=CFG[\"random_seed\"]\n",
    ").fit(E2)\n",
    "\n",
    "labels_seq = pd.DataFrame({\"customerid\": seq_emb[\"customerid\"], \"label\": km_seq.labels_})\n",
    "labels_seq.to_csv(Path(CFG[\"paths\"][\"processed_dir\"]) / \"embed_kmeans_labels_seq.csv\", index=False)\n",
    "\n",
    "profile_heatmap(features, labels_seq, \"cluster_profile_heatmap_seq.png\", \"Profiles — Sequence\")\n",
    "\n",
    "met_seq.to_csv(Path(CFG[\"paths\"][\"processed_dir\"]) / \"metrics_seq.csv\", index=False)\n",
    "print(\"SEQ clustering → best k =\", best_k_seq,\n",
    "      \"sil=\", met_seq.loc[met_seq[\"k\"]==best_k_seq, \"silhouette\"].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f986823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Stability (Seed Robustness)\n",
    "def sil_vs_seed(X, k=2):\n",
    "    vals = []\n",
    "    for seed in [1,7,13,21,42,99,1337]:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=seed).fit(X)\n",
    "        vals.append(silhouette_score(X, km.labels_))\n",
    "    return np.array(vals)\n",
    "\n",
    "sv_feat = sil_vs_seed(Xs_feat, k=best_k_feat)\n",
    "sv_ae   = sil_vs_seed(E,        k=best_k_ae)\n",
    "sv_seq  = sil_vs_seed(StandardScaler().fit_transform(E2), k=best_k_seq)\n",
    "\n",
    "plt.figure(figsize=(5.5,3.5))\n",
    "plt.boxplot([sv_feat, sv_ae, sv_seq], labels=[\"features\",\"ae\",\"seq\"])\n",
    "plt.ylabel(\"Silhouette\"); plt.title(\"Stability vs seed\")\n",
    "savefig(\"stability_box.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection (Isolation Forest + AE reconstruction MSE)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(random_state=CFG[\"random_seed\"], contamination=\"auto\")\n",
    "iso.fit(Xs_feat)\n",
    "if_score = -iso.score_samples(Xs_feat)  # higher = more anomalous\n",
    "\n",
    "# AE reconstruction error per customer (MSE)\n",
    "recon = ae.predict(Xs, verbose=0)\n",
    "ae_mse = ((Xs - recon)**2).mean(axis=1)\n",
    "\n",
    "anoms = pd.DataFrame({\n",
    "    \"customerid\": features[\"customerid\"],\n",
    "    \"if_score\": if_score,\n",
    "    \"ae_mse\": ae_mse,\n",
    "    \"spend_sum\": features[\"spend_sum\"],\n",
    "    \"tx_count\": features[\"tx_count\"],\n",
    "    \"basket_size_mean\": features[\"basket_size_mean\"],\n",
    "    \"recency_days\": features[\"recency_days\"],\n",
    "}).sort_values(\"if_score\", ascending=False)\n",
    "\n",
    "anoms.to_csv(PROC / \"anomaly_scores.csv\", index=False)\n",
    "anoms.head(10).to_csv(PROC / \"anomaly_top10.csv\", index=False)\n",
    "\n",
    "# Visuals: distributions + agreement + cume spend\n",
    "plt.hist(anoms[\"if_score\"], bins=40); plt.title(\"Isolation Forest scores\")\n",
    "savefig(\"if_score_hist.png\")\n",
    "\n",
    "plt.hist(anoms[\"ae_mse\"], bins=40); plt.title(\"AE reconstruction MSE\")\n",
    "savefig(\"ae_mse_hist.png\")\n",
    "\n",
    "plt.figure(figsize=(4.5,4))\n",
    "plt.scatter(anoms[\"if_score\"], anoms[\"ae_mse\"], s=10, alpha=0.5)\n",
    "plt.xlabel(\"IF score\"); plt.ylabel(\"AE MSE\"); plt.title(\"Anomaly agreement\")\n",
    "savefig(\"anomaly_agreement_scatter.png\")\n",
    "\n",
    "a = anoms.merge(features, on=\"customerid\")\n",
    "s = a.sort_values(\"if_score\", ascending=False)[\"spend_sum_x\"].to_numpy()\n",
    "cum = s.cumsum()/s.sum(); kgrid = np.arange(1, min(51, len(s)+1))\n",
    "plt.figure(figsize=(5,3.5)); plt.plot(kgrid, cum[:len(kgrid)])\n",
    "plt.xlabel(\"Top-K anomalies\"); plt.ylabel(\"Share of anomalous spend\")\n",
    "plt.title(\"Cumulative spend covered by Top-K anomalies\")\n",
    "savefig(\"anomaly_cume_spend.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054802d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports For What to drop into slides/poster\n",
    "export_list = [\n",
    "    \"daily_tx.png\",\"top_skus.png\",\"hists_core.png\",\"price_qty_hex.png\",\"pareto_spend.png\",\n",
    "    \"silhouette_k_features.png\",\"cluster_profile_heatmap_features.png\",\"segment_lift_spend.png\",\"segment_reengage_share.png\",\n",
    "    \"ae_loss.png\",\"silhouette_k_ae.png\",\"cluster_profile_heatmap_ae.png\",\n",
    "    \"lstm_loss.png\",\"silhouette_k_seq.png\",\"cluster_profile_heatmap_seq.png\",\n",
    "    \"stability_box.png\",\n",
    "    \"if_score_hist.png\",\"ae_mse_hist.png\",\"anomaly_agreement_scatter.png\",\"anomaly_cume_spend.png\",\n",
    "]\n",
    "missing = [f for f in export_list if not (FIGS/f).exists()]\n",
    "print(\"Figures ready:\", len(export_list)-len(missing), \" / \", len(export_list))\n",
    "if missing:\n",
    "    print(\"Missing:\", missing)\n",
    "\n",
    "print(\"\\nCSV/Parquet in:\", PROC)\n",
    "print(sorted([p.name for p in PROC.glob(\"*.csv\")]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
